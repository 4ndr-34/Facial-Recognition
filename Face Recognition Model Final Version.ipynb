{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "end_time": "2025-01-18T16:55:53.168594Z",
     "start_time": "2025-01-18T16:55:26.920185Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from datetime import datetime, timedelta\n",
    "from deepface import DeepFace\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename='face_recognition.log', level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Define the path to the ONNX model files\n",
    "face_detection_model_path = \"face_detection_yunet_2023mar.onnx\"\n",
    "face_recognition_model_path = \"face_recognition_sface_2021dec.onnx\"\n",
    "\n",
    "# Check if the ONNX model files exist\n",
    "if not os.path.exists(face_detection_model_path):\n",
    "    logging.error(f\"Face detection model file not found: {face_detection_model_path}\")\n",
    "    raise FileNotFoundError(f\"Face detection model file not found: {face_detection_model_path}\")\n",
    "\n",
    "if not os.path.exists(face_recognition_model_path):\n",
    "    logging.error(f\"Face recognition model file not found: {face_recognition_model_path}\")\n",
    "    raise FileNotFoundError(f\"Face recognition model file not found: {face_recognition_model_path}\")\n",
    "\n",
    "# Load the face detection and recognition models\n",
    "detector = cv.FaceDetectorYN.create(\n",
    "    face_detection_model_path,\n",
    "    \"\",\n",
    "    (320, 320),\n",
    "    0.7,\n",
    "    0.3,\n",
    "    5000,\n",
    "    backend_id=3\n",
    ")\n",
    "recognizer = cv.FaceRecognizerSF.create(\n",
    "    face_recognition_model_path, \"\", backend_id=3)\n",
    "\n",
    "# Set the cosine similarity threshold for face recognition\n",
    "cosine_similarity_threshold = 0.35\n",
    "\n",
    "# Define the path to the images directory\n",
    "path_to_images = \"Faces\"\n",
    "\n",
    "# Load the embeddings from the CSV file if it exists and is not empty, otherwise create an empty DataFrame\n",
    "embeddings_file = \"embeddings_df.csv\"\n",
    "if os.path.exists(embeddings_file) and os.path.getsize(embeddings_file) > 0:\n",
    "    embeddings_df = pd.read_csv(embeddings_file)\n",
    "    embeddings_df['Embedding'] = embeddings_df['Embedding'].apply(lambda x: np.array([float(t) for t in x.replace('[', '').replace(']', '').split()]))\n",
    "else:\n",
    "    embeddings_df = pd.DataFrame(columns=['Name', 'Embedding'])\n",
    "\n",
    "# Remove any rows with empty or incorrectly formatted embeddings\n",
    "embeddings_df = embeddings_df[embeddings_df['Embedding'].apply(lambda x: len(x) == 128)]\n",
    "\n",
    "# Set up Google Sheets credentials\n",
    "scope = ['https://www.googleapis.com/auth/drive']\n",
    "json_keyfile_path = r'face-recognition-new-credentials.json'  # Update with your file path\n",
    "\n",
    "# Check if the JSON keyfile exists\n",
    "if not os.path.exists(json_keyfile_path):\n",
    "    logging.error(f\"Google Sheets API credentials file not found: {json_keyfile_path}\")\n",
    "    raise FileNotFoundError(f\"Google Sheets API credentials file not found: {json_keyfile_path}\")\n",
    "\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name(json_keyfile_path, scope)\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "# Specify the Google Sheets document by its URL\n",
    "sheet_url = \"https://docs.google.com/spreadsheets/d/1Iqfx7UThkXlXW2fAvXWyEOejkNIQTQhsOaGqphVdCJ8/edit?gid=0#gid=0\"\n",
    "\n",
    "# Open the Google Sheets document\n",
    "doc = client.open_by_url(sheet_url)\n",
    "\n",
    "# Print out the names of all worksheets\n",
    "worksheets = doc.worksheets()\n",
    "print(\"Available worksheets:\")\n",
    "for worksheet in worksheets:\n",
    "    print(worksheet.title)\n",
    "\n",
    "# Try to get the worksheet named \"Face\"\n",
    "try:\n",
    "    sheet = doc.worksheet(\"Face\")\n",
    "    print('Worksheet \"Face\" found and accessed successfully.')\n",
    "except gspread.exceptions.WorksheetNotFound:\n",
    "    print('Worksheet \"Face\" not found. Creating a new one.')\n",
    "    sheet = doc.add_worksheet(title=\"Face\", rows=\"1000\", cols=\"20\")\n",
    "    sheet.append_row([\"Identity\", \"Timestamp\", \"Emotion\"])\n",
    "\n",
    "# Function to delete processed images from the \"Faces\" folder\n",
    "def delete_processed_images():\n",
    "    for filename in os.listdir(path_to_images):\n",
    "        image_name = filename[:-4]  # Remove the file extension\n",
    "        if image_name in embeddings_df['Name'].values:\n",
    "            os.remove(os.path.join(path_to_images, filename))\n",
    "            print(f\"Deleted processed image: {filename}\")\n",
    "\n",
    "# Function to generate embeddings for known faces\n",
    "def generate_embeddings():\n",
    "    global embeddings_df\n",
    "    for filename in os.listdir(path_to_images):\n",
    "        if filename.startswith(\"Unknown_\"):\n",
    "            continue  # Skip processing unknown face images\n",
    "        img_path = os.path.join(path_to_images, filename)\n",
    "        img = cv.imread(img_path)\n",
    "        if img is not None:\n",
    "            detector.setInputSize([img.shape[1], img.shape[0]])\n",
    "            face = detector.detect(img)\n",
    "            if face[1] is not None:\n",
    "                coords = face[1][0].astype(np.int32)\n",
    "                if (coords[2] >= 40) and (coords[3] >= 40):\n",
    "                    face_align = recognizer.alignCrop(img, face[1][0])\n",
    "                    face_feature = recognizer.feature(face_align)\n",
    "                    if face_feature.size == 128:  # Check for correct size\n",
    "                        x = {\"Name\": filename[:-4], \"Embedding\": face_feature}\n",
    "                        if filename[:-4] not in embeddings_df['Name'].values:\n",
    "                            embeddings_df = pd.concat(\n",
    "                                [\n",
    "                                    embeddings_df,\n",
    "                                    pd.DataFrame.from_dict([x], orient='columns')\n",
    "                                ],\n",
    "                                ignore_index=True)\n",
    "                    else:\n",
    "                        logging.warning(f\"Incorrect feature size for {filename}\")\n",
    "        else:\n",
    "            print(f\"Failed to read image: {img_path}\")\n",
    "\n",
    "# Save the updated embeddings to the CSV file\n",
    "def save_embeddings():\n",
    "    embeddings_df.to_csv(embeddings_file, index=False)\n",
    "\n",
    "# Function to visualize the detected faces, identities, and emotions\n",
    "def visualize(input, faces, identities=[], emotions=[], thickness=2):\n",
    "    if faces[1] is not None:\n",
    "        for idx, face in enumerate(faces[1]):\n",
    "            coords = face[:-1].astype(np.int32)\n",
    "            cv.rectangle(input, (coords[0], coords[1]), (coords[0]+coords[2], coords[1]+coords[3]), (0, 255, 0), thickness)\n",
    "            if identities:\n",
    "                cv.putText(input, identities[idx], (coords[0], coords[1]-10), cv.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "            if emotions:\n",
    "                cv.putText(input, emotions[idx], (coords[0], coords[1]+coords[3]+20), cv.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "\n",
    "# Open the video capture\n",
    "cap = cv.VideoCapture(0)\n",
    "\n",
    "new_resolution = 1\n",
    "\n",
    "# Dictionary to store the last detection timestamp and image count for each identity\n",
    "last_detection_time = {}\n",
    "image_count = {}\n",
    "\n",
    "# Dictionary to track detection statistics\n",
    "detection_stats = {\n",
    "    \"faces\": {},\n",
    "    \"persons\": {},\n",
    "    \"moods\": {}\n",
    "}\n",
    "\n",
    "# Function to determine if an image is blurry\n",
    "def is_blurry(image, threshold=100.0):\n",
    "    gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    laplacian_var = cv.Laplacian(gray, cv.CV_64F).var()\n",
    "    return laplacian_var < threshold\n",
    "\n",
    "# Function to update detection statistics\n",
    "def update_detection_stats(identity, emotion, timestamp):\n",
    "    if identity not in detection_stats[\"faces\"]:\n",
    "        detection_stats[\"faces\"][identity] = 0\n",
    "    detection_stats[\"faces\"][identity] += 1\n",
    "\n",
    "    if emotion not in detection_stats[\"moods\"]:\n",
    "        detection_stats[\"moods\"][emotion] = 0\n",
    "    detection_stats[\"moods\"][emotion] += 1\n",
    "\n",
    "    if timestamp not in detection_stats[\"persons\"]:\n",
    "        detection_stats[\"persons\"][timestamp] = {\"identity\": identity, \"count\": 0}\n",
    "    detection_stats[\"persons\"][timestamp][\"count\"] += 1\n",
    "\n",
    "# Function to get the most detected face, person, and mood\n",
    "def get_most_detected():\n",
    "    most_detected_face = max(detection_stats[\"faces\"], key=detection_stats[\"faces\"].get)\n",
    "    most_detected_mood = max(detection_stats[\"moods\"], key=detection_stats[\"moods\"].get)\n",
    "    most_detected_time = max(detection_stats[\"persons\"], key=lambda x: detection_stats[\"persons\"][x][\"count\"])\n",
    "    most_detected_person = detection_stats[\"persons\"][most_detected_time][\"identity\"]\n",
    "    return most_detected_face, most_detected_mood, most_detected_person, most_detected_time\n",
    "\n",
    "# Main loop for face recognition and emotion detection\n",
    "def main_loop():\n",
    "    global embeddings_df\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            pic_width = int(frame.shape[1] * new_resolution)\n",
    "            pic_height = int(frame.shape[0] * new_resolution)\n",
    "            new_dimension = (pic_width, pic_height)\n",
    "\n",
    "            frame = cv.resize(frame, new_dimension, interpolation=cv.INTER_AREA)\n",
    "\n",
    "            detector.setInputSize([frame.shape[1], frame.shape[0]])\n",
    "            faces = detector.detect(frame)\n",
    "\n",
    "            identities = []\n",
    "            emotions = []\n",
    "            if faces[1] is not None:\n",
    "                for i in range(len(faces[1])):\n",
    "                    coords = faces[1][i].astype(np.int32)\n",
    "                    if faces[1][i][-1] >= 0.7:\n",
    "                        face1_align = recognizer.alignCrop(frame, faces[1][i])\n",
    "                        if is_blurry(face1_align):\n",
    "                            continue  # Skip processing blurry images\n",
    "                        face1_feature = recognizer.feature(face1_align)\n",
    "                        identity = \"\"\n",
    "                        if face1_feature.size == 128:  # Ensure correct feature size\n",
    "                            embeddings_df['cosine'] = embeddings_df.Embedding.apply(\n",
    "                                lambda x: int(recognizer.match(face1_feature, np.reshape(x, face1_feature.shape).astype(np.float32), cv.FaceRecognizerSF_FR_COSINE) > cosine_similarity_threshold))\n",
    "                            a = embeddings_df[embeddings_df.cosine == 1]\n",
    "                            if len(a):\n",
    "                                identity = a.Name.iloc[0].capitalize()\n",
    "                            else:\n",
    "                                # Save the unknown face image in the \"Faces\" folder with limitation\n",
    "                                timestamp = datetime.now()\n",
    "                                if \"Unknown\" not in image_count:\n",
    "                                    image_count[\"Unknown\"] = 0\n",
    "                                    last_detection_time[\"Unknown\"] = timestamp\n",
    "                                if (timestamp - last_detection_time[\"Unknown\"]) < timedelta(minutes=1):\n",
    "                                    if image_count[\"Unknown\"] < 3:\n",
    "                                        unknown_filename = f\"Unknown_{timestamp.strftime('%Y%m%d_%H%M%S')}.jpg\"\n",
    "                                        unknown_path = os.path.join(path_to_images, unknown_filename)\n",
    "                                        cv.imwrite(unknown_path, face1_align)\n",
    "                                        image_count[\"Unknown\"] += 1\n",
    "                                else:\n",
    "                                    image_count[\"Unknown\"] = 1\n",
    "                                    last_detection_time[\"Unknown\"] = timestamp\n",
    "                                    unknown_filename = f\"Unknown_{timestamp.strftime('%Y%m%d_%H%M%S')}.jpg\"\n",
    "                                    unknown_path = os.path.join(path_to_images, unknown_filename)\n",
    "                                    cv.imwrite(unknown_path, face1_align)\n",
    "                                identity = \"Unknown\"\n",
    "                        else:\n",
    "                            logging.warning(\"Empty or incorrect feature vector for detected face.\")\n",
    "                            identity = \"Unknown\"\n",
    "\n",
    "                        identities.append(identity)\n",
    "\n",
    "                        # Detect emotion using DeepFace\n",
    "                        try:\n",
    "                            emotion_analysis = DeepFace.analyze(face1_align, actions=['emotion'], enforce_detection=False)\n",
    "                            if isinstance(emotion_analysis, list):\n",
    "                                if len(emotion_analysis) > 0 and 'dominant_emotion' in emotion_analysis[0]:\n",
    "                                    emotion = emotion_analysis[0]['dominant_emotion']\n",
    "                                else:\n",
    "                                    emotion = \"Unknown\"\n",
    "                            elif isinstance(emotion_analysis, dict) and 'dominant_emotion' in emotion_analysis:\n",
    "                                emotion = emotion_analysis['dominant_emotion']\n",
    "                            else:\n",
    "                                emotion = \"Unknown\"\n",
    "                        except Exception as e:\n",
    "                            logging.error(f\"Error in emotion analysis: {e}\")\n",
    "                            emotion = \"Unknown\"\n",
    "\n",
    "                        emotions.append(emotion)\n",
    "\n",
    "                        # Update detection statistics\n",
    "                        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                        update_detection_stats(identity, emotion, timestamp)\n",
    "\n",
    "                        # Store the detection details in Google Sheets if the time difference is at least 5 seconds\n",
    "                        current_time = datetime.now()\n",
    "                        if identity not in last_detection_time or (current_time - last_detection_time[identity]).total_seconds() >= 10:\n",
    "                            last_detection_time[identity] = current_time\n",
    "                            row = [identity, timestamp, emotion]\n",
    "                            try:\n",
    "                                logging.info(f\"Appending row to Google Sheets: {row}\")\n",
    "                                sheet.append_row(row)\n",
    "                                logging.info(\"Row appended successfully.\")\n",
    "                            except Exception as e:\n",
    "                                logging.error(f\"Failed to append row to Google Sheets: {e}\")\n",
    "\n",
    "            visualize(frame, faces, identities, emotions)\n",
    "\n",
    "            cv.imshow('frame', frame)\n",
    "            if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv.destroyAllWindows()\n",
    "\n",
    "    # Get and log the most detected face, person, and mood\n",
    "    most_detected_face, most_detected_mood, most_detected_person, most_detected_time = get_most_detected()\n",
    "    logging.info(f\"Most detected face: {most_detected_face}\")\n",
    "    logging.info(f\"Most detected mood: {most_detected_mood}\")\n",
    "    logging.info(f\"Most detected person: {most_detected_person} at {most_detected_time}\")\n",
    "\n",
    "    # Print the results to the console\n",
    "    print(f\"Most detected face: {most_detected_face}\")\n",
    "    print(f\"Most detected mood: {most_detected_mood}\")\n",
    "    print(f\"Most detected person: {most_detected_person} at {most_detected_time}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_embeddings()\n",
    "    save_embeddings()\n",
    "    delete_processed_images()\n",
    "    main_loop()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available worksheets:\n",
      "Sheet1\n",
      "Worksheet \"Main\" not found. Creating a new one.\n",
      "Failed to read image: Faces\\.DS_Store\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_12240\\3333595443.py\u001B[0m in \u001B[0;36m?\u001B[1;34m()\u001B[0m\n\u001B[0;32m    298\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0m__name__\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"__main__\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    299\u001B[0m     \u001B[0mgenerate_embeddings\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    300\u001B[0m     \u001B[0msave_embeddings\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    301\u001B[0m     \u001B[0mdelete_processed_images\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 302\u001B[1;33m     \u001B[0mmain_loop\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_12240\\3333595443.py\u001B[0m in \u001B[0;36m?\u001B[1;34m()\u001B[0m\n\u001B[0;32m    252\u001B[0m                             \u001B[1;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0memotion_analysis\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdict\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mand\u001B[0m \u001B[1;34m'dominant_emotion'\u001B[0m \u001B[1;32min\u001B[0m \u001B[0memotion_analysis\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    253\u001B[0m                                 \u001B[0memotion\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0memotion_analysis\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'dominant_emotion'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    254\u001B[0m                             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    255\u001B[0m                                 \u001B[0memotion\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"Unknown\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 256\u001B[1;33m                         \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    257\u001B[0m                             \u001B[0mlogging\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0merror\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"Error in emotion analysis: {e}\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    258\u001B[0m                             \u001B[0memotion\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"Unknown\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    259\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Downloads\\New Face Recognition Model\\New Face Recognition Model\\.venv\\Lib\\site-packages\\deepface\\DeepFace.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(img_path, actions, enforce_detection, detector_backend, align, expand_percentage, silent, anti_spoofing)\u001B[0m\n\u001B[0;32m    249\u001B[0m             \u001B[1;33m-\u001B[0m \u001B[1;34m'black'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mConfidence\u001B[0m \u001B[0mscore\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mBlack\u001B[0m \u001B[0methnicity\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    250\u001B[0m             \u001B[1;33m-\u001B[0m \u001B[1;34m'middle eastern'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mConfidence\u001B[0m \u001B[0mscore\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mMiddle\u001B[0m \u001B[0mEastern\u001B[0m \u001B[0methnicity\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    251\u001B[0m             \u001B[1;33m-\u001B[0m \u001B[1;34m'white'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mConfidence\u001B[0m \u001B[0mscore\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mWhite\u001B[0m \u001B[0methnicity\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    252\u001B[0m     \"\"\"\n\u001B[1;32m--> 253\u001B[1;33m     return demography.analyze(\n\u001B[0m\u001B[0;32m    254\u001B[0m         \u001B[0mimg_path\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mimg_path\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    255\u001B[0m         \u001B[0mactions\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mactions\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    256\u001B[0m         \u001B[0menforce_detection\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0menforce_detection\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Downloads\\New Face Recognition Model\\New Face Recognition Model\\.venv\\Lib\\site-packages\\deepface\\modules\\demography.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(img_path, actions, enforce_detection, detector_backend, align, expand_percentage, silent, anti_spoofing)\u001B[0m\n\u001B[0;32m    159\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    160\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0maction\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"emotion\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    161\u001B[0m                 emotion_predictions = modeling.build_model(\n\u001B[0;32m    162\u001B[0m                     \u001B[0mtask\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"facial_attribute\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel_name\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"Emotion\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 163\u001B[1;33m                 ).predict(img_content)\n\u001B[0m\u001B[0;32m    164\u001B[0m                 \u001B[0msum_of_predictions\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0memotion_predictions\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    165\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    166\u001B[0m                 \u001B[0mobj\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"emotion\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m{\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Downloads\\New Face Recognition Model\\New Face Recognition Model\\.venv\\Lib\\site-packages\\deepface\\models\\demography\\Emotion.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m     53\u001B[0m         \u001B[0mimg_gray\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexpand_dims\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimg_gray\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     54\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     55\u001B[0m         \u001B[1;31m# model.predict causes memory issue when it is called in a for loop\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     56\u001B[0m         \u001B[1;31m# emotion_predictions = self.model.predict(img_gray, verbose=0)[0, :]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 57\u001B[1;33m         \u001B[0memotion_predictions\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimg_gray\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtraining\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     58\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     59\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0memotion_predictions\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Downloads\\New Face Recognition Model\\New Face Recognition Model\\.venv\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     68\u001B[0m             \u001B[1;31m# To get the full stack trace, call:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     69\u001B[0m             \u001B[1;31m# `tf.debugging.disable_traceback_filtering()`\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     70\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     71\u001B[0m         \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 72\u001B[1;33m             \u001B[1;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\Downloads\\New Face Recognition Model\\New Face Recognition Model\\.venv\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    584\u001B[0m                 \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__call__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0mcopied_args\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mcopied_kwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    585\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    586\u001B[0m             \u001B[0mlayout_map_lib\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_map_subclass_model_variable\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_layout_map\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    587\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 588\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__call__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\Downloads\\New Face Recognition Model\\New Face Recognition Model\\.venv\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     68\u001B[0m             \u001B[1;31m# To get the full stack trace, call:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     69\u001B[0m             \u001B[1;31m# `tf.debugging.disable_traceback_filtering()`\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     70\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     71\u001B[0m         \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 72\u001B[1;33m             \u001B[1;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\Downloads\\New Face Recognition Model\\New Face Recognition Model\\.venv\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1138\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1139\u001B[0m                 with autocast_variable.enable_auto_cast_variables(\n\u001B[0;32m   1140\u001B[0m                     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_compute_dtype_object\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1141\u001B[0m                 ):\n\u001B[1;32m-> 1142\u001B[1;33m                     \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcall_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1143\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1144\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_activity_regularizer\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1145\u001B[0m                     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_handle_activity_regularization\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moutputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Downloads\\New Face Recognition Model\\New Face Recognition Model\\.venv\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    154\u001B[0m                 \u001B[0mnew_e\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    155\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[0mnew_e\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    156\u001B[0m         \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    157\u001B[0m             \u001B[1;32mdel\u001B[0m \u001B[0msignature\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 158\u001B[1;33m             \u001B[1;32mdel\u001B[0m \u001B[0mbound_signature\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\Downloads\\New Face Recognition Model\\New Face Recognition Model\\.venv\\Lib\\site-packages\\tf_keras\\src\\engine\\sequential.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(self, inputs, training, mask)\u001B[0m\n\u001B[0;32m    393\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    394\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_graph_initialized\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    395\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbuilt\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    396\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_init_graph_network\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moutputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 397\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcall\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtraining\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtraining\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmask\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmask\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    398\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    399\u001B[0m         \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0minputs\u001B[0m  \u001B[1;31m# handle the corner case where self.layers is empty\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    400\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mlayer\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Downloads\\New Face Recognition Model\\New Face Recognition Model\\.venv\\Lib\\site-packages\\tf_keras\\src\\engine\\functional.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(self, inputs, training, mask)\u001B[0m\n\u001B[0;32m    510\u001B[0m         \u001B[0mReturns\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    511\u001B[0m             \u001B[0mA\u001B[0m \u001B[0mtensor\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mthere\u001B[0m \u001B[1;32mis\u001B[0m \u001B[0ma\u001B[0m \u001B[0msingle\u001B[0m \u001B[0moutput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mor\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    512\u001B[0m             \u001B[0ma\u001B[0m \u001B[0mlist\u001B[0m \u001B[0mof\u001B[0m \u001B[0mtensors\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mthere\u001B[0m \u001B[0mare\u001B[0m \u001B[0mmore\u001B[0m \u001B[0mthan\u001B[0m \u001B[0mone\u001B[0m \u001B[0moutputs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    513\u001B[0m         \"\"\"\n\u001B[1;32m--> 514\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_run_internal_graph\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtraining\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtraining\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmask\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmask\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\Downloads\\New Face Recognition Model\\New Face Recognition Model\\.venv\\Lib\\site-packages\\tf_keras\\src\\engine\\functional.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(self, inputs, training, mask)\u001B[0m\n\u001B[0;32m    667\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0many\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mt_id\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtensor_dict\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mt_id\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mnode\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mflat_input_ids\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    668\u001B[0m                     \u001B[1;32mcontinue\u001B[0m  \u001B[1;31m# Node is not computable, try skipping.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    669\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    670\u001B[0m                 \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnode\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmap_arguments\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensor_dict\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 671\u001B[1;33m                 \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnode\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    672\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    673\u001B[0m                 \u001B[1;31m# Update tensor_dict.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    674\u001B[0m                 for x_id, y in zip(\n",
      "\u001B[1;32m~\\Downloads\\New Face Recognition Model\\New Face Recognition Model\\.venv\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     68\u001B[0m             \u001B[1;31m# To get the full stack trace, call:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     69\u001B[0m             \u001B[1;31m# `tf.debugging.disable_traceback_filtering()`\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     70\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     71\u001B[0m         \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 72\u001B[1;33m             \u001B[1;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\Downloads\\New Face Recognition Model\\New Face Recognition Model\\.venv\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1138\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1139\u001B[0m                 with autocast_variable.enable_auto_cast_variables(\n\u001B[0;32m   1140\u001B[0m                     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_compute_dtype_object\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1141\u001B[0m                 ):\n\u001B[1;32m-> 1142\u001B[1;33m                     \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcall_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1143\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1144\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_activity_regularizer\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1145\u001B[0m                     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_handle_activity_regularization\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moutputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Downloads\\New Face Recognition Model\\New Face Recognition Model\\.venv\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    154\u001B[0m                 \u001B[0mnew_e\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    155\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[0mnew_e\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    156\u001B[0m         \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    157\u001B[0m             \u001B[1;32mdel\u001B[0m \u001B[0msignature\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 158\u001B[1;33m             \u001B[1;32mdel\u001B[0m \u001B[0mbound_signature\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\Downloads\\New Face Recognition Model\\New Face Recognition Model\\.venv\\Lib\\site-packages\\tf_keras\\src\\layers\\convolutional\\base_conv.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(self, inputs)\u001B[0m\n\u001B[0;32m    285\u001B[0m             outputs = self._jit_compiled_convolution_op(\n\u001B[0;32m    286\u001B[0m                 \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconvert_to_tensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkernel\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    287\u001B[0m             )\n\u001B[0;32m    288\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 289\u001B[1;33m             \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconvolution_op\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkernel\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    290\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    291\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0muse_bias\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    292\u001B[0m             \u001B[0moutput_rank\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0moutputs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrank\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Downloads\\New Face Recognition Model\\New Face Recognition Model\\.venv\\Lib\\site-packages\\tf_keras\\src\\layers\\convolutional\\base_conv.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(self, inputs, kernel)\u001B[0m\n\u001B[0;32m    257\u001B[0m             \u001B[0mtf_padding\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    258\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    259\u001B[0m             \u001B[0mtf_padding\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    260\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 261\u001B[1;33m         return tf.nn.convolution(\n\u001B[0m\u001B[0;32m    262\u001B[0m             \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    263\u001B[0m             \u001B[0mkernel\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    264\u001B[0m             \u001B[0mstrides\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstrides\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Downloads\\New Face Recognition Model\\New Face Recognition Model\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    151\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    152\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    153\u001B[0m       \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    154\u001B[0m     \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 155\u001B[1;33m       \u001B[1;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\Downloads\\New Face Recognition Model\\New Face Recognition Model\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m   1257\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1258\u001B[0m       \u001B[1;31m# Fallback dispatch system (dispatch v1):\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1259\u001B[0m       \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1260\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mdispatch_target\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1261\u001B[1;33m       \u001B[1;32mexcept\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mTypeError\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1262\u001B[0m         \u001B[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1263\u001B[0m         \u001B[1;31m# TypeError, when given unexpected types.  So we need to catch both.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1264\u001B[0m         \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdispatch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mop_dispatch_handler\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Downloads\\New Face Recognition Model\\New Face Recognition Model\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(input, filters, strides, padding, data_format, dilations, name)\u001B[0m\n\u001B[0;32m   1182\u001B[0m     \u001B[0mpadding\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"VALID\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1183\u001B[0m     \u001B[0mdata_format\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1184\u001B[0m     \u001B[0mdilations\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1185\u001B[0m     name=None):\n\u001B[1;32m-> 1186\u001B[1;33m   return convolution_internal(\n\u001B[0m\u001B[0;32m   1187\u001B[0m       \u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m  \u001B[1;31m# pylint: disable=redefined-builtin\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1188\u001B[0m       \u001B[0mfilters\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1189\u001B[0m       \u001B[0mstrides\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mstrides\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Downloads\\New Face Recognition Model\\New Face Recognition Model\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(input, filters, strides, padding, data_format, dilations, name, call_from_convolution, num_spatial_dims)\u001B[0m\n\u001B[0;32m   1315\u001B[0m         \u001B[0mop\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_conv3d_expanded_batch\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1316\u001B[0m       \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1317\u001B[0m         \u001B[0mop\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mconv1d\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1318\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1319\u001B[1;33m       return op(\n\u001B[0m\u001B[0;32m   1320\u001B[0m           \u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1321\u001B[0m           \u001B[0mfilters\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1322\u001B[0m           \u001B[0mstrides\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Downloads\\New Face Recognition Model\\New Face Recognition Model\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(input, filters, strides, padding, data_format, dilations, name)\u001B[0m\n\u001B[0;32m   2789\u001B[0m   \u001B[0minput_rank\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrank\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2790\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[0minput_rank\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0minput_rank\u001B[0m \u001B[1;33m<\u001B[0m \u001B[1;36m5\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2791\u001B[0m     \u001B[1;31m# We avoid calling squeeze_batch_dims to reduce extra python function\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2792\u001B[0m     \u001B[1;31m# call slowdown in eager mode.  This branch doesn't require reshapes.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2793\u001B[1;33m     return gen_nn_ops.conv2d(\n\u001B[0m\u001B[0;32m   2794\u001B[0m         \u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2795\u001B[0m         \u001B[0mfilter\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mfilters\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2796\u001B[0m         \u001B[0mstrides\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mstrides\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Downloads\\New Face Recognition Model\\New Face Recognition Model\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001B[0m\n\u001B[0;32m   1458\u001B[0m         \"dilations\", dilations)\n\u001B[0;32m   1459\u001B[0m       \u001B[1;32mreturn\u001B[0m \u001B[0m_result\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1460\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0m_core\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1461\u001B[0m       \u001B[0m_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mraise_from_not_ok_status\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1462\u001B[1;33m     \u001B[1;32mexcept\u001B[0m \u001B[0m_core\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_FallbackException\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1463\u001B[0m       \u001B[1;32mpass\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1464\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1465\u001B[0m       return conv2d_eager_fallback(\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "cda529ae373f75ed",
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "end_time": "2025-01-28T22:23:13.513211Z",
     "start_time": "2025-01-28T22:23:11.369085Z"
    }
   },
   "source": [
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up Google Sheets credentials\n",
    "scope = ['https://www.googleapis.com/auth/drive']\n",
    "json_keyfile_path = r'face-recognition-new-credentials.json'  # Update with your file path\n",
    "\n",
    "# Check if the JSON keyfile exists\n",
    "if not os.path.exists(json_keyfile_path):\n",
    "    raise FileNotFoundError(f\"Google Sheets API credentials file not found: {json_keyfile_path}\")\n",
    "\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name(json_keyfile_path, scope)\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "# Specify the Google Sheets document by its URL\n",
    "sheet_url = \"https://docs.google.com/spreadsheets/d/1Iqfx7UThkXlXW2fAvXWyEOejkNIQTQhsOaGqphVdCJ8/edit?gid=0#gid=0\"\n",
    "\n",
    "# Open the Google Sheets document\n",
    "doc = client.open_by_url(sheet_url)\n",
    "\n",
    "# Try to get the worksheet named \"Face\"\n",
    "try:\n",
    "    sheet = doc.worksheet(\"Face\")\n",
    "except gspread.exceptions.WorksheetNotFound:\n",
    "    raise FileNotFoundError('Worksheet \"Face\" not found.')\n",
    "\n",
    "# Load the data from Google Sheets into a pandas DataFrame\n",
    "data = sheet.get_all_records()\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert the Timestamp column to datetime\n",
    "df['Time Stamp'] = pd.to_datetime(df['Time Stamp'])\n",
    "\n",
    "# Plot the number of detections over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "df.set_index('Time Stamp').resample('H').size().plot()\n",
    "plt.title('Number of Detections Over Time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Number of Detections')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot the most detected persons\n",
    "plt.figure(figsize=(10, 6))\n",
    "df['Identity'].value_counts().plot(kind='bar')\n",
    "plt.title('Most Detected Persons')\n",
    "plt.xlabel('Person')\n",
    "plt.ylabel('Number of Detections')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot the most detected moods\n",
    "plt.figure(figsize=(10, 6))\n",
    "df['Emotion'].value_counts().plot(kind='bar')\n",
    "plt.title('Most Detected Moods')\n",
    "plt.xlabel('Mood')\n",
    "plt.ylabel('Number of Detections')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 11\u001B[0m\n\u001B[0;32m      8\u001B[0m json_keyfile_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mface-recognition-new-credentials.json\u001B[39m\u001B[38;5;124m'\u001B[39m  \u001B[38;5;66;03m# Update with your file path\u001B[39;00m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# Check if the JSON keyfile exists\u001B[39;00m\n\u001B[1;32m---> 11\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[43mos\u001B[49m\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(json_keyfile_path):\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGoogle Sheets API credentials file not found: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mjson_keyfile_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     14\u001B[0m creds \u001B[38;5;241m=\u001B[39m ServiceAccountCredentials\u001B[38;5;241m.\u001B[39mfrom_json_keyfile_name(json_keyfile_path, scope)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'os' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14d5a7215124cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
